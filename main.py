# -*- coding: utf-8 -*-
"""
Civitaiå›¾ç‰‡çˆ¬è™«ä¸»æµç¨‹ï¼šåªç”¨æ ¸å¿ƒæ»šåŠ¨ä»£ç ï¼ŒæŠ“å–æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯å¹¶ä¿å­˜åˆ°Excel
åŒæ—¶æ•´åˆtestElementTree.pyä¸­çš„å…ƒç´ å…³ç³»åˆ†æžï¼Œæ‰¾å‡ºimgå’Œbuttonçš„å…±åŒç¥–å…ˆè·¯å¾„ã€‚
æ–°å¢žæ›´æ™ºèƒ½çš„ç»“æž„è¯†åˆ«ï¼Œæ—¨åœ¨ç²¾å‡†å®šä½â€œå›¾ç‰‡-è¯„è®ºâ€å•å…ƒçš„å…±åŒç¥–å…ˆã€‚
ç‰¹åˆ«å¢žåŠ äº†å¯¹â€œç‚¹èµžæ•°ã€çˆ±å¿ƒæ•°ã€ç¬‘å“­æ•°ã€ä¼¤å¿ƒæ•°ã€æ‰“èµæ•°â€è¿™5ä¸ªåŒå±‚çº§æŒ‰é’®çš„ç²¾å‡†å®šä½å’Œæ•°æ®æå–ã€‚
æ ¹æ®ç”¨æˆ·æä¾›çš„HTMLæ–‡ä»¶ï¼ˆall.html, box.html, 5buttons.htmlï¼‰æ›´æ–°äº†é€‰æ‹©å™¨ã€‚
"""

import os
import json
import asyncio
import aiohttp
from datetime import datetime
import traceback
import logging
import subprocess
from openpyxl import Workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.hyperlink import Hyperlink
import hashlib
import aiofiles
import re
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import playwright._impl._errors
import time

# --- é…ç½® ---
PROXY = "http://127.0.0.1:10808"
TARGET_URL = "https://civitai.com/images?tags=4" # This will be overridden by KEYWORD_TARGET_FILE if used
LOG_DIR = "logs"
RESULTS_DIR = "results_civitai"
IMAGE_DIR_BASE = "images_civitai"
KEYWORD_TARGET_FILE = "urlTarget.txt"
DOWNLOAD_HISTORY_FILE = "download_history_civitai.json"

timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
log_filename = os.path.join(LOG_DIR, f"civitai_scraper_log_{timestamp}.txt")
excel_filename = os.path.join(RESULTS_DIR, f"civitai_image_results_{timestamp}.xlsx")
# å®šä¹‰ç”¨äºŽå…ƒç´ è·¯å¾„åˆ†æžçš„æ—¥å¿—å’ŒExcelæ–‡ä»¶
ELEMENT_LOG_FILE = os.path.join(LOG_DIR, f"element_analysis_log_{timestamp}.txt")
ELEMENT_XLSX_FILE = os.path.join(RESULTS_DIR, f"element_common_ancestors_{timestamp}.xlsx")


# --- æ—¥å¿—é…ç½® ---
logger = logging.getLogger('civitai_scraper')
logger.setLevel(logging.INFO)
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
if not os.path.exists(RESULTS_DIR):
    os.makedirs(RESULTS_DIR)
if not os.path.exists(IMAGE_DIR_BASE):
    os.makedirs(IMAGE_DIR_BASE)
file_handler = logging.FileHandler(log_filename, encoding='utf-8')
console_handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# --- å·¥å…·å‡½æ•° ---
def calculate_md5(data_bytes):
    return hashlib.md5(data_bytes).hexdigest()

def load_download_history(filepath):
    global download_history
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                download_history = json.load(f)
            logger.info(f"Loaded download history from {filepath}")
        except Exception as e:
            logger.warning(f"Failed to load download history: {e}")
            download_history = {}
    else:
        logger.info(f"Download history file '{filepath}' not found. Starting with empty history.")
        download_history = {}

def save_download_history(filepath):
    global download_history
    try:
        dir_name = os.path.dirname(filepath)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(download_history, f, indent=4)
        logger.info(f"Saved download history to {filepath}")
    except Exception as e:
        logger.error(f"Error saving download history to '{filepath}': {e}\n{traceback.format_exc()}")

def read_urls_from_file(filepath):
    urls = []
    if not os.path.exists(filepath):
        logger.error(f"Error: Target URL file '{filepath}' not found. è¯·åˆ›å»ºå¹¶æ·»åŠ URLã€‚")
        return []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and url.startswith("http"):
                    urls.append(url)
        if not urls:
            logger.warning(f"Warning: Target URL file '{filepath}' is empty or contains no valid URLs.")
        return urls
    except Exception as e:
        logger.error(f"Error reading URLs from '{filepath}': {e}\n{traceback.format_exc()}")
        return []

async def process_image_data(image_url, base_folder_path):
    local_filename = None
    image_content_md5 = None
    image_bytes = None
    if not image_url:
        logger.warning("Empty image URL skipped.")
        return None, None
    if image_url.startswith('http'):
        url_without_query = image_url.split('?')[0]
        file_extension = url_without_query.split('.')[-1].lower()
        if not file_extension or len(file_extension) > 5 or not file_extension.isalpha():
            file_extension = 'jpg'
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(image_url, proxy=PROXY if PROXY else None, timeout=30.0) as response:
                    response.raise_for_status()
                    image_bytes = await response.read()
                    image_content_md5 = calculate_md5(image_bytes)
                    if image_content_md5 in download_history:
                        existing_path = download_history[image_content_md5]
                        logger.info(f"Downloaded image content (MD5: {image_content_md5}) already exists at: {existing_path}. Skipping save and using existing path.")
                        return existing_path, image_content_md5
                    local_filename = os.path.join(base_folder_path, f"{image_content_md5}.{file_extension}")
                    os.makedirs(os.path.dirname(local_filename), exist_ok=True)
                    async with aiofiles.open(local_filename, 'wb') as f:
                        await f.write(image_bytes)
                    logger.info(f"Image downloaded and saved: {local_filename}")
                    download_history[image_content_md5] = local_filename
                    return local_filename, image_content_md5
        except Exception as e:
            logger.error(f"Error downloading image {image_url}: {e}")
    else:
        logger.warning(f"Unsupported image URL format (not http/https): {image_url[:100]}...")
    return None, None

# --- ä»Ž testElementTree.py å¤åˆ¶å¹¶ä¿®æ”¹çš„å‡½æ•° ---
def get_element_path(element, ancestor):
    """
    èŽ·å–å…ƒç´ ç›¸å¯¹äºŽæŒ‡å®šç¥–å…ˆå…ƒç´ çš„è·¯å¾„ã€‚
    ä¾‹å¦‚ï¼šdiv > span(2) > img
    """
    path = []
    current = element
    # ç¡®ä¿currentå’Œancestoréƒ½æ˜¯æœ‰æ•ˆçš„BeautifulSoup Tagå¯¹è±¡
    if not current or not ancestor or not hasattr(current, 'name'):
        return ""

    while current and current != ancestor:
        tag = current.name
        if not tag: # å¤„ç†æ²¡æœ‰tagçš„NavigableStringç­‰
            break
        # é¿å…æ— é™å¾ªçŽ¯ï¼Œå¦‚æžœ current å·²ç»æ²¡æœ‰çˆ¶çº§ä½†ä»æœªè¾¾åˆ° ancestor
        if not current.parent:
            break

        # è®¡ç®—åŒçº§åŒåå…ƒç´ çš„ç´¢å¼•
        siblings = [sib for sib in current.parent.find_all(tag, recursive=False)]
        if len(siblings) > 1:
            try:
                idx = siblings.index(current) + 1
                tag = f"{tag}({idx})"
            except ValueError: # å¦‚æžœ current ä¸åœ¨ siblings ä¸­ (ä¸åº”è¯¥å‘ç”Ÿä½†ä¸ºäº†å¥å£®æ€§)
                pass
        path.insert(0, tag)
        current = current.parent
    return " > ".join(path)

def get_common_prefix(paths):
    """
    æ‰¾æ‰€æœ‰è·¯å¾„çš„æœ€é•¿å…¬å…±å‰ç¼€ï¼ˆä»¥' > 'ä¸ºåˆ†éš”ï¼‰ã€‚
    paths åº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ [path_string]ã€‚
    """
    if not paths:
        return ""
    # å°†è·¯å¾„å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºåˆ—è¡¨
    split_paths = [p[0].split(" > ") for p in paths if p and p[0]]
    if not split_paths:
        return ""

    min_len = min(len(p) for p in split_paths)
    prefix = []
    for i in range(min_len):
        # æ£€æŸ¥å½“å‰å±‚çº§çš„æ‰€æœ‰è·¯å¾„æ˜¯å¦ç›¸åŒ
        tokens = set(p[i] for p in split_paths)
        if len(tokens) == 1:
            prefix.append(tokens.pop())
        else:
            # å¦‚æžœä¸åŒï¼Œåˆ™å…¬å…±å‰ç¼€åˆ°æ­¤ä¸ºæ­¢
            break
    return " > ".join(prefix)

def get_elements_and_paths(ancestor, tag_name):
    """
    é€šç”¨å‡½æ•°ï¼Œç”¨äºŽèŽ·å–æŒ‡å®šæ ‡ç­¾åä¸‹æ‰€æœ‰å…ƒç´ çš„è·¯å¾„ï¼ˆç›¸å¯¹ancestorï¼‰ã€‚
    è¿”å›žä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ [path_string]ã€‚
    """
    paths = []
    # æŸ¥æ‰¾ancestorä¸‹çš„æ‰€æœ‰tag_nameå…ƒç´ 
    for elem in ancestor.find_all(tag_name):
        paths.append([get_element_path(elem, ancestor)])
    return paths

# --- æ ¸å¿ƒçˆ¬è™«æµç¨‹ ---
all_search_results_data = []
data_lock = asyncio.Lock()
download_history = {}
# å­˜å‚¨é¡µé¢HTMLå†…å®¹ï¼Œä»¥ä¾¿åŽç»­è¿›è¡Œå…ƒç´ åˆ†æž
global_page_html = None


async def performCivitaiImageScrape(context, target_url):
    async_name = asyncio.current_task().get_name()
    # --- Cookie æ³¨å…¥ ---
    try:
        if os.path.exists("cookies.json"):
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
                for cookie in cookies:
                    cookie_same_site = {'strict': 'Strict', 'Lax': 'Lax', 'none': 'None'}.get(cookie.get('sameSite'), None)
                    if cookie_same_site in ['Strict', 'Lax', 'None']:
                        cookie['sameSite'] = cookie_same_site
                    else:
                        if 'sameSite' in cookie:
                            del cookie['sameSite']
                await context.add_cookies(cookies)
            logger.info(f"{async_name} -> Cookies loaded and added to context.")
        else:
            logger.warning(f"{async_name} -> Warning: cookies.json not found. Proceeding without cookies.")
    except Exception as e:
        logger.error(f"{async_name} -> Error loading cookies: {e}")

    page = await context.new_page()
    try:
        logger.info(f"{async_name} -> Navigating to {target_url}")
        await page.goto(target_url, timeout=60000, wait_until="domcontentloaded")
        logger.info(f"{async_name} -> Successfully navigated to {target_url}")
    except Exception as e:
        logger.error(f"{async_name} -> Error navigating to {target_url}: {e}")
        await page.close()
        return

    civitai_image_folder_path = os.path.join(IMAGE_DIR_BASE, "downloaded_images")
    if not os.path.exists(civitai_image_folder_path):
        os.makedirs(civitai_image_folder_path)
        logger.info(f"{async_name} -> Created base image folder for Civitai: {civitai_image_folder_path}")

    scrape_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    processed_image_detail_urls = set()  # æ”¾åœ¨ while å¾ªçŽ¯å¤–
    scroll_attempts = 0
    #ï¼ï¼ï¼ï¼ï¼
    max_scroll_attempts = 50  # è°ƒè¯•æ˜¯50ï¼Œä¹‹åŽå†åŠ å¤§æ»šåŠ¨æ¬¡æ•°
    no_new_images_start_time = None  # æ–°å¢žï¼šæ— æ–°å›¾ç‰‡å¼€å§‹çš„æ—¶é—´

    # æ ¹æ®all.htmlæ–‡ä»¶æ›´æ–°ï¼šå¤§çš„å®¹å™¨ 'all'
    main_content_area_selector = 'div.mx-auto.flex.justify-center.gap-4'

    keyword_input_selector = 'header input'  # å…³é”®è¯è¾“å…¥æ¡†é€‰æ‹©å™¨
    current_keyword = "N/A"
    try:
        keyword_input_element = page.locator(keyword_input_selector)
        if await keyword_input_element.is_visible():
            input_value = await keyword_input_element.get_attribute('value')
            if input_value:
                current_keyword = input_value
                logger.info(f"{async_name} -> Found keyword in input field: '{current_keyword}'")
    except Exception as e:
        logger.warning(f"{async_name} -> Could not find or extract keyword: {e}")

    while scroll_attempts < max_scroll_attempts:
        scroll_attempts += 1
        logger.info(f"{async_name} -> Scroll attempt {scroll_attempts}/{max_scroll_attempts}...")
        # ä¼˜åŒ–æ»šåŠ¨ï¼Œå°è¯•æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(0.05) # çŸ­æš‚ç­‰å¾…ç¡®ä¿æ»šåŠ¨å®Œæˆ

        # åœ¨æ¯æ¬¡æ»šåŠ¨åŽèŽ·å–æœ€æ–°HTML
        page_html = await page.content()
        soup = BeautifulSoup(page_html, "html.parser")

        # æ‰¾åˆ°åŒ…å«æ‰€æœ‰å›¾ç‰‡ç›’å­çš„ä¸»è¦å†…å®¹åŒºåŸŸ 'all'
        target_container = soup.select_one(main_content_area_selector)
        if not target_container:
            logger.warning(f"ä¸»å†…å®¹å®¹å™¨æœªæ‰¾åˆ°ï¼Œé€‰æ‹©å™¨: {main_content_area_selector}ã€‚è·³è¿‡æœ¬æ¬¡å¾ªçŽ¯ã€‚")
            continue

        # åœ¨ä¸»å†…å®¹å®¹å™¨å†…ï¼Œè¯†åˆ«æ¯ä¸ªç‹¬ç«‹çš„â€œå›¾ç‰‡-è¯„è®ºâ€å•å…ƒ 'box'
        # æ ¹æ®box.htmlæ–‡ä»¶æ›´æ–°ï¼šæ¯ä¸ªå›¾ç‰‡å¡ç‰‡ 'box'
        image_comment_units = target_container.find_all(
            "div", class_="relative flex overflow-hidden rounded-md border-gray-3 bg-gray-0 shadow-gray-4 dark:border-dark-4 dark:bg-dark-6 dark:shadow-dark-8 flex-col border"
        )
        
        current_image_count = len(image_comment_units) # ç»Ÿè®¡è¯†åˆ«åˆ°çš„å›¾ç‰‡è¯„è®ºå•å…ƒæ•°é‡
        newly_processed_this_scroll = 0

        for unit in image_comment_units:
            img = unit.find("img") # imgåœ¨boxå†…éƒ¨
            if not img:
                continue

            thumbnail_url = img.get("src")
            if not thumbnail_url or not thumbnail_url.startswith("http"):
                continue

            parent_a = img.find_parent("a")
            original_page_url = parent_a.get("href") if parent_a else ""
            if original_page_url and not original_page_url.startswith("http"):
                original_page_url = f"https://civitai.com{original_page_url}"

            # åŽ»é‡
            unique_key = thumbnail_url + "|" + original_page_url
            if unique_key in processed_image_detail_urls:
                continue
            processed_image_detail_urls.add(unique_key)
            newly_processed_this_scroll += 1

            # --- å¼€å§‹æå–5ä¸ªæŒ‰é’®çš„æ•°æ® ---
            likes_count = "N/A"
            love_count = "N/A"
            laugh_count = "N/A"
            sad_count = "N/A"
            tipped_count = "N/A"

            # æ ¹æ®5buttons.htmlæ–‡ä»¶æ›´æ–°ï¼š5ä¸ªæŒ‰é’®çš„å…±åŒçˆ¶çº§å…ƒç´ 
            buttons_parent = unit.select_one("div.flex.items-center.justify-center.gap-1.justify-between.p-2")

            if buttons_parent:
                buttons = buttons_parent.find_all("button")
                for btn in buttons:
                    # æå–æ•°å­—ï¼Œå¹¶å¤„ç†å¯èƒ½å­˜åœ¨çš„Kã€Mç­‰å•ä½
                    def parse_count_text(text):
                        if not text:
                            return "0"
                        text = text.strip().replace(',', '')
                        if 'K' in text:
                            return str(int(float(text.replace('K', '')) * 1000))
                        elif 'M' in text:
                            return str(int(float(text.replace('M', '')) * 1000000))
                        try:
                            # ç¡®ä¿è¿”å›žçš„æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—
                            return str(int(text))
                        except ValueError:
                            return "0"

                    # å°è¯•ä»Žmantine-Button-labelå†…éƒ¨çš„æ–‡æœ¬æå–
                    count_label_span = btn.find("span", class_="mantine-qo1k2 flex gap-1 mantine-Button-label")
                    count_value = "0"
                    if count_label_span:
                        # æ‰¾åˆ°æ•°å­—æ‰€åœ¨çš„æ–‡æœ¬èŠ‚ç‚¹ï¼Œæ•°å­—é€šå¸¸ä¸åœ¨ç‹¬ç«‹çš„spanä¸­ï¼Œè€Œæ˜¯åœ¨çˆ¶spançš„ç›´æŽ¥æ–‡æœ¬ä¸­
                        # æˆ–è€…åœ¨ mantine-Text-root mantine-9yukw3 æ—è¾¹çš„æ–‡æœ¬
                        # é’ˆå¯¹ 5buttons.html å‘çŽ°æ•°å­—åœ¨ `mantine-Button-label` è¿™ä¸ª span ä¹‹åŽï¼Œæˆ–è€…ç›´æŽ¥ä½œä¸ºå…¶æ–‡æœ¬
                        # æ›´å¥½çš„åŠžæ³•æ˜¯å¯»æ‰¾æ•°å­—æ—è¾¹çš„æ–‡æœ¬èŠ‚ç‚¹
                        
                        # å°è¯•æå–ç›´æŽ¥æ–‡æœ¬ï¼Œä¾‹å¦‚ "ðŸ‘ 12111"
                        full_text = count_label_span.get_text(strip=True)
                        match = re.search(r'(\d+(\.\d+)?[KM]?)$', full_text) # åŒ¹é…æœ«å°¾çš„æ•°å­—æˆ–å¸¦KMçš„æ•°å­—
                        if match:
                            count_value = parse_count_text(match.group(1))
                        elif count_label_span.parent and count_label_span.parent.get_text(strip=True):
                            # æœ‰æ—¶æ•°å­—åœ¨ mantine-Button-inner çš„å­æ–‡æœ¬ä¸­ï¼Œè€Œä¸æ˜¯ label å†…éƒ¨
                            parent_text = count_label_span.parent.get_text(strip=True)
                            match = re.search(r'(\d+(\.\d+)?[KM]?)$', parent_text)
                            if match:
                                count_value = parse_count_text(match.group(1))
                                
                    # ç‰¹æ®Šå¤„ç†æ‰“èµæŒ‰é’®ï¼Œå®ƒçš„æ•°å­—åœ¨ mantine-Badge-label ä¸­
                    badge_label_span = btn.find("span", class_="mantine-h9iq4m flex gap-0.5 items-center mantine-Badge-inner")
                    if badge_label_span:
                        # æ‰¾åˆ°svgåŽçš„æ–‡æœ¬ï¼Œæˆ–è€…ç›´æŽ¥åœ¨badge_label_spanä¸­æå–
                        # è¿™é‡Œéœ€è¦æ›´ç²¾ç¡®åœ°æ‰¾åˆ°æ•°å­—ï¼Œä¾‹å¦‚ä»Žspançš„å­æ–‡æœ¬ä¸­
                        badge_text = badge_label_span.get_text(strip=True)
                        match = re.search(r'(\d+(\.\d+)?[KM]?)$', badge_text)
                        if match:
                            count_value = parse_count_text(match.group(1))
                        
                    # æ ¹æ®å›¾æ ‡æˆ– aria-label/data-tooltip è¯†åˆ«æŒ‰é’®ç±»åž‹
                    # 5buttons.html ä¸­æ²¡æœ‰ data-tooltipï¼Œä½†æœ‰ emoji
                    icon_div = btn.find("div", class_="mantine-Text-root mantine-9yukw3")
                    if icon_div:
                        icon_text = icon_div.get_text(strip=True)
                        if icon_text == 'ðŸ‘':
                            likes_count = count_value
                        elif icon_text == 'â¤ï¸':
                            love_count = count_value
                        elif icon_text == 'ðŸ˜‚':
                            laugh_count = count_value
                        elif icon_text == 'ðŸ˜¢':
                            sad_count = count_value
                    # æ‰“èµæŒ‰é’®æ²¡æœ‰ç›´æŽ¥çš„emojiï¼Œé€šå¸¸æ˜¯svgå›¾æ ‡
                    # å¯ä»¥æ ¹æ®å…¶ class æˆ– aria-label æ¥åˆ¤æ–­ï¼Œæˆ–è€…å®ƒæ˜¯ç¬¬äº”ä¸ªæŒ‰é’®
                    # åœ¨ 5buttons.html ä¸­ï¼Œæ‰“èµæŒ‰é’®çš„ç±»åæ˜¯ mantine-1qn9423
                    if "mantine-1qn9423" in btn.get("class", []):
                        tipped_count = count_value


            # --- æå–ç»“æŸ ---

            # ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚éœ€åŠ é€Ÿå¯æ³¨é‡ŠæŽ‰ï¼ŒåŽç»­æ‰¹é‡ä¸‹è½½ï¼‰
            local_image_path, image_md5 = await process_image_data(thumbnail_url, civitai_image_folder_path)
            if local_image_path:
                abs_path = os.path.abspath(local_image_path)
                if os.name == 'nt':
                    abs_path = abs_path.replace('\\', '/')
                    local_image_hyperlink = f"file:///{abs_path}"
                else:
                    local_image_hyperlink = f"file://{abs_path}"
            else:
                local_image_hyperlink = ""
            result_data = {
                "æŠ“å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "æœç´¢URL": target_url,
                "ç¼©ç•¥å›¾URL": thumbnail_url,
                "æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„": os.path.abspath(local_image_path) if local_image_path else "",
                "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æŽ¥": local_image_hyperlink,
                "åŽŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æŽ¥": original_page_url,
                "ç‚¹èµžæ•°": likes_count, # æ·»åŠ ç‚¹èµžæ•°
                "çˆ±å¿ƒæ•°": love_count,   # æ·»åŠ çˆ±å¿ƒæ•°
                "ç¬‘å“­æ•°": laugh_count, # æ·»åŠ ç¬‘å“­æ•°
                "ä¼¤å¿ƒæ•°": sad_count,   # æ·»åŠ ä¼¤å¿ƒæ•°
                "æ‰“èµæ•°": tipped_count, # æ·»åŠ æ‰“èµæ•°
                "å…³é”®è¯": current_keyword
            }
            async with data_lock:
                all_search_results_data.append(result_data)

        if newly_processed_this_scroll == 0 and current_image_count > 0:
            if no_new_images_start_time is None:
                no_new_images_start_time = time.time()
            elapsed = time.time() - no_new_images_start_time
            logger.info(f"{async_name} -> No new images processed this scroll. Consecutive no new images: {no_new_images_count}, elapsed: {elapsed:.1f}s")
            # æŒç»­20ç§’æ²¡æœ‰æ–°å›¾ç‰‡åˆ™åœæ­¢æ»šåŠ¨
            if elapsed >= 20:
                logger.info(f"{async_name} -> No new images for 20 seconds. Stopping scrolling.")
                break
        else:
            no_new_images_start_time = None  # æœ‰æ–°å›¾ç‰‡å°±é‡ç½®
            no_new_images_count = 0 # é‡ç½®è®¡æ•°
    await page.close()
    logger.info(f"{async_name} -> Page closed for {target_url}.")

    # åœ¨æ‰€æœ‰ scraping ä»»åŠ¡å®ŒæˆåŽï¼Œå†è¿›è¡Œ HTML é¡µé¢å†…å®¹çš„å…¨å±€ä¿å­˜å’Œå…ƒç´ åˆ†æž
    # è¿™é‡Œå‡è®¾æˆ‘ä»¬åªåˆ†æžæœ€åŽä¸€ä¸ªæˆåŠŸæŠ“å–åˆ°çš„é¡µé¢çš„HTML
    global global_page_html
    global_page_html = page_html # ä¿å­˜æœ€åŽä¸€æ¬¡èŽ·å–çš„é¡µé¢HTML


# --- å…ƒç´ ç»“æž„åˆ†æžå‡½æ•° ---
async def analyze_civitai_element_structure(page_html):
    element_analysis_errors = []
    element_analysis_results = {} # å­˜å‚¨åˆ†æžç»“æžœ

    if not page_html:
        logger.warning("No page HTML content provided for element analysis. Skipping analysis.")
        return element_analysis_results, element_analysis_errors

    try:
        soup = BeautifulSoup(page_html, "html.parser")
        
        # è¯†åˆ«ä¸»å†…å®¹åŒºåŸŸçš„ç¥–å…ˆé€‰æ‹©å™¨ 'all'
        main_content_area_selector = 'div.mx-auto.flex.justify-center.gap-4'
        ancestor_for_elements = soup.select_one(main_content_area_selector)

        if not ancestor_for_elements:
            element_analysis_errors.append(f"Error: Ancestor element for analysis not found with selector: {main_content_area_selector}")
        else:
            # è¯†åˆ«æ¯ä¸ªç‹¬ç«‹çš„â€œå›¾ç‰‡-è¯„è®ºâ€å•å…ƒ 'box'
            image_comment_units = ancestor_for_elements.find_all(
                "div", class_="relative flex overflow-hidden rounded-md border-gray-3 bg-gray-0 shadow-gray-4 dark:border-dark-4 dark:bg-dark-6 dark:shadow-dark-8 flex-col border"
            )

            if not image_comment_units:
                element_analysis_errors.append(f"Warning: No 'image-comment' units found with selector 'div.relative.flex.overflow-hidden...' under the main ancestor.")
                
            unit_paths = []
            all_img_paths_in_units = []
            all_button_group_paths_in_units = [] # è®°å½•æŒ‰é’®ç»„çš„è·¯å¾„

            for unit in image_comment_units:
                unit_path = get_element_path(unit, ancestor_for_elements) # boxç›¸å¯¹äºŽall
                unit_paths.append([unit_path])

                # åœ¨æ¯ä¸ª unit å†…éƒ¨æŸ¥æ‰¾ img
                img_in_unit = unit.find("img")
                if img_in_unit:
                    all_img_paths_in_units.append([get_element_path(img_in_unit, unit)]) # imgç›¸å¯¹äºŽbox

                # åœ¨æ¯ä¸ª unit å†…éƒ¨æŸ¥æ‰¾5ä¸ªæŒ‰é’®çš„å…±åŒçˆ¶çº§å…ƒç´ 
                buttons_parent = unit.select_one("div.flex.items-center.justify-center.gap-1.justify-between.p-2") # 5button parentç›¸å¯¹äºŽbox
                if buttons_parent:
                    all_button_group_paths_in_units.append([get_element_path(buttons_parent, unit)]) # æŒ‰é’®ç»„ç›¸å¯¹äºŽbox


            # è®¡ç®—å„ä¸ªå±‚çº§çš„å…±åŒç¥–å…ˆè·¯å¾„
            common_ancestor_for_all_units = get_common_prefix(unit_paths) # boxçš„å…±åŒç¥–å…ˆè·¯å¾„ (ç›¸å¯¹äºŽall)
            common_relative_path_for_imgs_in_units = get_common_prefix(all_img_paths_in_units) # imgåœ¨boxå†…çš„å…±åŒè·¯å¾„
            common_relative_path_for_button_groups_in_units = get_common_prefix(all_button_group_paths_in_units) # æŒ‰é’®ç»„åœ¨boxå†…çš„å…±åŒè·¯å¾„

            element_analysis_results = {
                "Common Ancestor for Image-Comment Units (Box in All)": common_ancestor_for_all_units,
                "Common Relative Path for Images within Units (Img in Box)": common_relative_path_for_imgs_in_units,
                "Common Relative Path for Button Groups within Units (5ButtonParent in Box)": common_relative_path_for_button_groups_in_units,
                "All Image-Comment Unit Paths (Box Relative to All)": [p[0] for p in unit_paths],
                "All Relative Image Paths within Units (Img Relative to Box)": [p[0] for p in all_img_paths_in_units],
                "All Relative Button Group Paths within Units (5ButtonParent Relative to Box)": [p[0] for p in all_button_group_paths_in_units]
            }

    except Exception as e:
        element_analysis_errors.append(f"An unexpected error occurred during element analysis: {e}\n{traceback.format_exc()}")
        logger.error(f"Error during element analysis: {e}")
        
    return element_analysis_results, element_analysis_errors


# --- ä¸»å…¥å£ ---
async def main():
    load_download_history(DOWNLOAD_HISTORY_FILE)
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            proxy={"server": PROXY} if PROXY else None,
            timeout=60000
        )
        context = await browser.new_context(
            viewport={'width': 2560, 'height': 1440}
        )
        target_urls = read_urls_from_file(KEYWORD_TARGET_FILE)
        if not target_urls:
            logger.error(f"No valid URLs found in {KEYWORD_TARGET_FILE}. Please add URLs to scrape.")
            await browser.close()
            return
        # ç›´æŽ¥æŠ“å–æ‰€æœ‰URL
        tasks = [performCivitaiImageScrape(context, url) for url in target_urls]
        await asyncio.gather(*tasks) # æ‰§è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡
        await browser.close()
        logger.info("Browser closed. Script finished scraping data.")

    # --- å…ƒç´ ç»“æž„åˆ†æžéƒ¨åˆ† (åœ¨æ‰€æœ‰çˆ¬è™«ä»»åŠ¡ç»“æŸåŽæ‰§è¡Œä¸€æ¬¡) ---
    element_analysis_results, element_analysis_errors = await analyze_civitai_element_structure(global_page_html)

    # å†™ log
    with open(ELEMENT_LOG_FILE, "w", encoding="utf-8") as f:
        if element_analysis_errors:
            f.write("\n\n".join(element_analysis_errors))
        else:
            f.write("Element analysis completed successfully.\n")
            for key, value in element_analysis_results.items():
                if isinstance(value, list):
                    f.write(f"\n{key}:\n")
                    for item in value:
                        f.write(f"  - {item}\n")
                else:
                    f.write(f"\n{key}: {value}\n")

    # å†™ xlsx
    wb_elements = Workbook()
    ws_elements = wb_elements.active
    ws_elements.title = "Element Commonalities"
    
    ws_elements.append(["Analysis Category", "Value"])
    ws_elements.append(["Common Ancestor for Image-Comment Units (Box in All)", element_analysis_results.get("Common Ancestor for Image-Comment Units (Box in All)", "N/A")])
    ws_elements.append(["Common Relative Path for Images within Units (Img in Box)", element_analysis_results.get("Common Relative Path for Images within Units (Img in Box)", "N/A")])
    ws_elements.append(["Common Relative Path for Button Groups within Units (5ButtonParent in Box)", element_analysis_results.get("Common Relative Path for Button Groups within Units (5ButtonParent in Box)", "N/A")])

    # è¯¦ç»†è·¯å¾„åˆ—è¡¨
    ws_elements.append([]) # Blank row
    ws_elements.append(["Detailed Paths"])
    ws_elements.append(["Image-Comment Unit Paths (Box Relative to All)"])
    for path in element_analysis_results.get("All Image-Comment Unit Paths (Box Relative to All)", []):
        ws_elements.append([path])
    
    ws_elements.append([]) # Blank row
    ws_elements.append(["Relative Image Paths within Units (Img Relative to Box)"])
    for path in element_analysis_results.get("All Relative Image Paths within Units (Img Relative to Box)", []):
        ws_elements.append([path])

    ws_elements.append([]) # Blank row
    ws_elements.append(["Relative Button Group Paths within Units (5ButtonParent Relative to Box)"])
    for path in element_analysis_results.get("All Relative Button Group Paths within Units (5ButtonParent Relative to Box)", []):
        ws_elements.append([path])

    wb_elements.save(ELEMENT_XLSX_FILE)
    logger.info(f"Element common ancestors saved to Excel: {ELEMENT_XLSX_FILE}")

    # è‡ªåŠ¨æ‰“å¼€ log å’Œ xlsx
    try:
        os.startfile(ELEMENT_LOG_FILE)
    except Exception:
        subprocess.Popen(['notepad.exe', ELEMENT_LOG_FILE])
    try:
        os.startfile(ELEMENT_XLSX_FILE)
    except Exception:
        subprocess.Popen(['start', ELEMENT_XLSX_FILE], shell=True)


    # --- Excel å¯¼å‡º (ä¸»çˆ¬è™«æ•°æ®ï¼Œå¢žåŠ æ–°çš„åˆ—) ---
    wb = Workbook()
    ws = wb.active
    ws.title = "Civitaiå›¾ç‰‡ç»“æžœ"
    # æ›´æ–°Headersï¼Œå¢žåŠ æ–°çš„åˆ—
    headers = ["æŠ“å–æ—¶é—´", "æœç´¢URL", "ç¼©ç•¥å›¾URL", "æœ¬åœ°ç¼©ç•¥å›¾è·¯å¾„", "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æŽ¥", "åŽŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æŽ¥", "ç‚¹èµžæ•°", "çˆ±å¿ƒæ•°", "ç¬‘å“­æ•°", "ä¼¤å¿ƒæ•°", "æ‰“èµæ•°", "å…³é”®è¯"]
    ws.append(headers)
    hyperlink_font = Font(color="0000FF", underline="single")
    for row_data in all_search_results_data:
        row = []
        for header in headers:
            if header == "æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æŽ¥":
                row.append("ç‚¹å‡»æ‰“å¼€ç¼©ç•¥å›¾")
            else:
                row.append(row_data.get(header, ""))
        ws.append(row)
        current_row_idx = ws.max_row
        search_url = row_data.get("æœç´¢URL")
        if search_url:
            cell_search_url = ws.cell(row=current_row_idx, column=headers.index("æœç´¢URL") + 1)
            cell_search_url.value = search_url
            cell_search_url.hyperlink = search_url
            cell_search_url.font = hyperlink_font
        thumbnail_url = row_data.get("ç¼©ç•¥å›¾URL")
        if thumbnail_url:
            cell_thumbnail_url = ws.cell(row=current_row_idx, column=headers.index("ç¼©ç•¥å›¾URL") + 1)
            cell_thumbnail_url.value = thumbnail_url
            cell_thumbnail_url.hyperlink = thumbnail_url
            cell_thumbnail_url.font = hyperlink_font
        local_image_hyperlink_url = row_data.get("æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æŽ¥")
        if local_image_hyperlink_url:
            cell_local_image_hyperlink = ws.cell(row=current_row_idx, column=headers.index("æœ¬åœ°ç¼©ç•¥å›¾è¶…é“¾æŽ¥") + 1)
            cell_local_image_hyperlink.value = "ç‚¹å‡»æ‰“å¼€ç¼©ç•¥å›¾"
            cell_local_image_hyperlink.hyperlink = Hyperlink(ref=local_image_hyperlink_url)
            cell_local_image_hyperlink.font = hyperlink_font
        original_page_link = row_data.get("åŽŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æŽ¥")
        if original_page_link:
            cell_original_page_link = ws.cell(row=current_row_idx, column=headers.index("åŽŸå§‹å›¾ç‰‡è¯¦æƒ…é¡µé“¾æŽ¥") + 1)
            cell_original_page_link.value = original_page_link
            cell_original_page_link.hyperlink = original_page_link
            cell_original_page_link.font = hyperlink_font
    for col_idx, header in enumerate(headers):
        max_length = len(header)
        column_letter = get_column_letter(col_idx + 1)
        for r_idx in range(1, ws.max_row + 1):
            cell_value = ws.cell(row=r_idx, column=col_idx + 1).value
            if cell_value:
                cell_len = len(str(cell_value))
                if cell_len > max_length:
                    max_length = cell_len
        adjusted_width = (max_length + 2) * 1.2
        if adjusted_width > 100:
            adjusted_width = 100
        ws.column_dimensions[column_letter].width = adjusted_width
    wb.save(excel_filename)
    logger.info(f"Results saved to Excel: {excel_filename}")
    save_download_history(DOWNLOAD_HISTORY_FILE)

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Script interrupted by user.")
    except Exception as e:
        logger.critical(f"An unhandled error occurred in main: {e}\n{traceback.format_exc()}")
    finally:
        # Closing log handlers and opening files logic from main.py
        for handler in logger.handlers[:]:
            try:
                handler.flush()
                handler.close()
                logger.removeHandler(handler)
            except Exception as e:
                print(f"Error closing log handler: {e}")
        try:
            if os.path.exists(log_filename):
                print(f"Attempting to open log file: {log_filename}")
                if os.name == 'nt':
                    os.startfile(log_filename)
                elif hasattr(os, "uname") and os.uname().sysname == 'Darwin':
                    subprocess.run(['open', log_filename])
                else:
                    subprocess.run(['xdg-open', log_filename])
            else:
                print(f"Log file not found: {log_filename}")
        except Exception as e:
            print(f"Error opening log file {log_filename}: {e}")
        try:
            if os.path.exists(excel_filename):
                print(f"Attempting to open Excel file: {excel_filename}")
                if os.name == 'nt':
                    os.startfile(excel_filename)
                elif hasattr(os, "uname") and os.uname().sysname == 'Darwin':
                    subprocess.run(['open', excel_filename])
                else:
                    subprocess.run(['xdg-open', excel_filename])
            else:
                print(f"Excel file not found: {excel_filename}")
        except Exception as e:
            print(f"Error opening Excel file {excel_filename}: {e}")